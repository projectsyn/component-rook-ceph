parameters:
  rook_ceph:
    =_metadata: {}
    namespace: syn-rook-ceph-operator

    ceph_cluster:
      name: cluster
      namespace: syn-rook-ceph-${rook_ceph:ceph_cluster:name}
      node_count: 3
      block_storage_class: localblock
      # Configure volume size here, if block storage PVs are provisioned
      # dynamically
      block_volume_size: 1
      # set to true if backing storage is SSD
      tune_fast_device_class: false
      # Control placement of osd pods.
      osd_placement: {}

      # Rendered into rook-config-override CM
      config_override:
        global:
          # Configure full ratios to match the alerts shipped with Rook.
          # With this config the cluster goes readonly at 85% utilization.
          # These configs only apply at cluster creation.
          # To adjust the ratios at run time, use
          #   `ceph osd set-{nearfull,backfillfull,full}-ratio`
          # NOTE: we're giving ratios as strings to avoid float rounding
          # issues when manifesting the values in the resulting config file.
          mon_osd_full_ratio: '0.85'
          mon_osd_backfillfull_ratio: '0.8'
          mon_osd_nearfull_ratio: '0.75'
        osd:
          # We explicitly set bluefs_buffered_io to false to get good write
          # bandwidth on Exoscale -> TODO: needs to be checked per
          # infrastructure, maybe move the config to cloud/exoscale/params.yml
          bluefs_buffered_io: false

      # Whether to setup RBD CSI driver and pools
      rbd_enabled: true
      # Whether to setup CephFS CSI driver and pools
      cephfs_enabled: false
      # Whether to enable monitoring
      monitoring_enabled: true
      # Ceph alerts to ignore
      # The component supports removal of entries from this array by
      # giving the entry prefixed with `~` (same syntax as for the
      # applications array).
      ignore_alerts:
        # Disable Ceph pool quota alerts by default
        - CephPoolQuotaBytesNearExhaustion
        - CephPoolQuotaBytesCriticallyExhausted

      storage_pools:
        rbd:
          storagepool:
            config:
              failureDomain: host
              replicated:
                size: 3
                requireSafeReplicaSize: true
            mount_options:
              discard: true
            storage_class_config:
              parameters:
                csi.storage.k8s.io/fstype: ext4
              allowVolumeExpansion: true
        cephfs:
          fspool:
            data_pools:
              pool0:
                failureDomain: host
                replicated:
                  size: 3
                  requireSafeReplicaSize: true
                parameters:
                  compression_mode: none
            config:
              metadataPool:
                replicated:
                  size: 3
                  requireSafeReplicaSize: true
              parameters:
                compression_mode: none
              # dataPools rendered from data_pools in Jsonnet
              preserveFilesystemOnDelete: true
              metadataServer:
                activeCount: 1
                activeStandby: true
                # metadata server placement done in Jsonnet but can be
                # extended here
              mirroring:
                enabled: false
            mount_options:
              discard: true
            storage_class_config:
              allowVolumeExpansion: true


    node_selector:
      node-role.kubernetes.io/storage: ''

    tolerations:
      - key: storagenode
        operator: Exists

    images:
      rook:
        registry: docker.io
        image: rook/ceph
        tag: v1.7.4
      ceph:
        registry: docker.io
        image: ceph/ceph
        tag: v16.2.5
      cephcsi:
        registry: quay.io
        image: cephcsi/cephcsi
        tag: v3.4.0

    charts:
      # We do not support helm chart versions older than v1.7.0
      rook-ceph: v1.7.4

    operator_helm_values:
      image:
        repository: ${rook_ceph:images:rook:registry}/${rook_ceph:images:rook:image}
        tag: ${rook_ceph:images:rook:tag}
      nodeSelector: ${rook_ceph:node_selector}
      resources:
        limits:
          cpu: 1000m
          memory: 1Gi
        requests:
          cpu: 750m
          memory: 512Mi
      tolerations: ${rook_ceph:tolerations}
      csi:
        enableCSIHostNetwork: false
        enableRbdDriver: ${rook_ceph:ceph_cluster:rbd_enabled}
        enableCephfsDriver: ${rook_ceph:ceph_cluster:cephfs_enabled}
        enableGrpcMetrics: true
        cephcsi:
          image: ${rook_ceph:images:cephcsi:registry}/${rook_ceph:images:cephcsi:image}:${rook_ceph:images:cephcsi:tag}

    toolbox:
      enabled: true
      image: ${rook_ceph:images:rook}

    cephClusterSpec:
      cephVersion:
        image: ${rook_ceph:images:ceph:registry}/${rook_ceph:images:ceph:image}:${rook_ceph:images:ceph:tag}
        allowUnsupported: false
      dataDirHostPath: /var/lib/rook
      monitoring:
        enabled: ${rook_ceph:ceph_cluster:monitoring_enabled}
        rulesNamespace: ${rook_ceph:ceph_cluster:namespace}
      mon:
        count: 3
        allowMultiplePerNode: false
      network:
        provider: host
      placement:
        all:
          # nodeAffinity is injected in Jsonnet,
          # taking placement labels from ${rook_ceph:node_selector}
          tolerations: ${rook_ceph:tolerations}
      resources:
        mgr:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
        mon:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
        osd:
          limits:
            cpu: "6"
            memory: 5Gi
          requests:
            cpu: "4"
            memory: 5Gi
      storage:
        useAllNodes: false
        useAllDevices: false
        storageClassDeviceSets:
          - name: ${rook_ceph:ceph_cluster:name}
            count: ${rook_ceph:ceph_cluster:node_count}
            volumeClaimTemplates:
              - spec:
                  storageClassName: ${rook_ceph:ceph_cluster:block_storage_class}
                  volumeMode: Block
                  accessModes:
                    - ReadWriteOnce
                  resources:
                    requests:
                      storage: ${rook_ceph:ceph_cluster:block_volume_size}
            encrypted: true
            tuneFastDeviceClass: ${rook_ceph:ceph_cluster:tune_fast_device_class}
            placement: ${rook_ceph:ceph_cluster:osd_placement}

      disruptionManagement:
        managePodBudgets: true
        osdMaintenanceTimeout: 30
