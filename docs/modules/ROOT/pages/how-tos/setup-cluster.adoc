= Setup a PVC-based Ceph cluster

NOTE: For specific instructions to setup a Ceph cluster running on OpenShift 4 on Exoscale see the VSHN OpenShift 4 Exoscale https://kb.vshn.ch/oc4/how-tos/exoscale/install.html[cluster installation] how-to

This how-to assumes that you're setting up a PVC-based Ceph cluster on an infrastructure which has a CSI driver which supports both `volumeMode: Block` and volume expansion.

== Prerequisites

* A local setup to compile a cluster catalog, see https://syn.tools/commodore/explanation/running-commodore.html[Running Commodore] for details.
* An access token for the Lieutenant instance on which your cluster is registered.

This guide assumes that you're familiar with making changes to a Project Syn cluster and compiling the cluster catalog to deploy those changes.

== Steps

. Enable component `rook-ceph` for your cluster
+
[source,yaml]
----
applications:
  - rook-ceph
----

. Configure component `rook_ceph` to setup the cluster by requesting PVCs from the CSI driver
+
[source,yaml]
----
parameters:
  rook_ceph:
    ceph_cluster:
      block_storage_class: ssd <1>
      block_volume_size: 500Gi <2>
      tune_fast_device_class: true <3>
      osd_portable: true <4>
      osd_placement: <5>
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: ScheduleAnyway
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - rook-ceph-osd
----
<1> The name of the storage class to use to provision the OSD PVCs
<2> The size of each OSD PVC
<3> Tune Ceph for SSD or better backing storage.
Set this to `false` if you're provisioning your cluster on a storage class which is backed by HDDs.
<4> Tell Rook to configure Ceph to not permanently assign each OSD to a specific cluster node.
This allows cluster nodes to be replaced without having to recreate the OSDs hosted on the decommissioned node.
<5> Configure the OSDs to be scheduled across the available nodes.
By using `topologySpreadConstraints`, we're allowing multiple OSDs to be scheduled on the same node as long as the node is big enough while still ensuring that the OSDs are evenly spread across the available nodes.
+
See the xref:reference/parameters.adoc[component parameters] reference documentation for the full set of available configuration options.

. Compile the cluster catalog
. Wait until the cluster is provisioned
