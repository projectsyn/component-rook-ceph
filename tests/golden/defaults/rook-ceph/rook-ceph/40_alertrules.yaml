apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app.kubernetes.io/component: rook-ceph
    app.kubernetes.io/managed-by: commodore
    app.kubernetes.io/name: syn-prometheus-ceph-rules
    prometheus: rook-prometheus
    role: alert-rules
  name: syn-prometheus-ceph-rules
  namespace: syn-rook-ceph-cluster
spec:
  groups:
    - name: ceph-mgr-status
      rules:
        - alert: SYN_CephMgrIsAbsent
          annotations:
            description: Ceph Manager has disappeared from Prometheus target discovery.
            message: Storage metrics collector service not available anymore.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephMgrIsAbsent.html
            severity_level: critical
            storage_type: ceph
          expr: 'label_replace((up{job="rook-ceph-mgr"} == 0 or absent(up{job="rook-ceph-mgr"})),
            "namespace", "openshift-storage", "", "")

            '
          for: 5m
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
    - name: ceph-mds-status
      rules:
        - alert: SYN_CephMdsMissingReplicas
          annotations:
            description: Minimum required replicas for storage metadata service not
              available. Might affect the working of storage cluster.
            message: Insufficient replicas for storage metadata service.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephMdsMissingReplicas.html
            severity_level: warning
            storage_type: ceph
          expr: 'sum(ceph_mds_metadata{job="rook-ceph-mgr"} == 1) by (namespace) <
            2

            '
          for: 5m
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
    - name: quorum-alert.rules
      rules:
        - alert: SYN_CephMonQuorumAtRisk
          annotations:
            description: Storage cluster quorum is low. Contact Support.
            message: Storage quorum at risk
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephMonQuorumAtRisk.html
            severity_level: error
            storage_type: ceph
          expr: 'count(ceph_mon_quorum_status{job="rook-ceph-mgr"} == 1) by (namespace)
            <= (floor(count(ceph_mon_metadata{job="rook-ceph-mgr"}) by (namespace)
            / 2) + 1)

            '
          for: 15m
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephMonQuorumLost
          annotations:
            description: Storage cluster quorum is lost. Contact Support.
            message: Storage quorum is lost
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephMonQuorumLost.html
            severity_level: critical
            storage_type: ceph
          expr: 'count(kube_pod_status_phase{pod=~"rook-ceph-mon-.*", phase=~"Running|running"}
            == 1) by (namespace) < 2

            '
          for: 5m
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephMonHighNumberOfLeaderChanges
          annotations:
            description: Ceph Monitor {{ $labels.ceph_daemon }} on host {{ $labels.hostname
              }} has seen {{ $value | printf "%.2f" }} leader changes per minute recently.
            message: Storage Cluster has seen many leader changes recently.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephMonHighNumberOfLeaderChanges.html
            severity_level: warning
            storage_type: ceph
          expr: '(ceph_mon_metadata{job="rook-ceph-mgr"} * on (ceph_daemon) group_left()
            (rate(ceph_mon_num_elections{job="rook-ceph-mgr"}[5m]) * 60)) > 0.95

            '
          for: 5m
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
    - name: osd-alert.rules
      rules:
        - alert: SYN_CephOSDCriticallyFull
          annotations:
            description: Utilization of storage device {{ $labels.ceph_daemon }} of
              device_class type {{$labels.device_class}} has crossed 80% on host {{
              $labels.hostname }}. Immediately free up some space or add capacity
              of type {{$labels.device_class}}.
            message: Back-end storage device is critically full.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephOSDCriticallyFull.html
            severity_level: error
            storage_type: ceph
          expr: '(ceph_osd_metadata * on (ceph_daemon) group_right(device_class,hostname)
            (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes)) >= 0.80

            '
          for: 40s
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephOSDFlapping
          annotations:
            description: Storage daemon {{ $labels.ceph_daemon }} has restarted 5
              times in last 5 minutes. Please check the pod events or ceph status
              to find out the cause.
            message: Ceph storage osd flapping.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephOSDFlapping.html
            severity_level: error
            storage_type: ceph
          expr: 'changes(ceph_osd_up[5m]) >= 10

            '
          for: 0s
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephOSDNearFull
          annotations:
            description: Utilization of storage device {{ $labels.ceph_daemon }} of
              device_class type {{$labels.device_class}} has crossed 75% on host {{
              $labels.hostname }}. Immediately free up some space or add capacity
              of type {{$labels.device_class}}.
            message: Back-end storage device is nearing full.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephOSDNearFull.html
            severity_level: warning
            storage_type: ceph
          expr: '(ceph_osd_metadata * on (ceph_daemon) group_right(device_class,hostname)
            (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes)) >= 0.75

            '
          for: 40s
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephOSDDiskNotResponding
          annotations:
            description: Disk device {{ $labels.device }} not responding, on host
              {{ $labels.host }}.
            message: Disk not responding
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephOSDDiskNotResponding.html
            severity_level: error
            storage_type: ceph
          expr: 'label_replace((ceph_osd_in == 1 and ceph_osd_up == 0),"disk","$1","ceph_daemon","osd.(.*)")
            + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation,"host","$1","exported_instance","(.*)")

            '
          for: 5m
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephOSDDiskUnavailable
          annotations:
            description: Disk device {{ $labels.device }} not accessible on host {{
              $labels.host }}.
            message: Disk not accessible
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephOSDDiskUnavailable.html
            severity_level: error
            storage_type: ceph
          expr: 'label_replace((ceph_osd_in == 0 and ceph_osd_up == 0),"disk","$1","ceph_daemon","osd.(.*)")
            + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation,"host","$1","exported_instance","(.*)")

            '
          for: 1m
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephOSDSlowOps
          annotations:
            description: '{{ $value }} Ceph OSD requests are taking too long to process.
              Please check ceph status to find out the cause.'
            message: OSD requests are taking too long to process.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephOSDSlowOps.html
            severity_level: warning
            storage_type: ceph
          expr: 'ceph_healthcheck_slow_ops > 0

            '
          for: 30s
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephDataRecoveryTakingTooLong
          annotations:
            description: Data recovery has been active for too long. Contact Support.
            message: Data recovery is slow
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephDataRecoveryTakingTooLong.html
            severity_level: warning
            storage_type: ceph
          expr: 'ceph_pg_undersized > 0

            '
          for: 2h
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephPGRepairTakingTooLong
          annotations:
            description: Self heal operations taking too long. Contact Support.
            message: Self heal problems detected
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephPGRepairTakingTooLong.html
            severity_level: warning
            storage_type: ceph
          expr: 'ceph_pg_inconsistent > 0

            '
          for: 1h
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
    - name: cluster-state-alert.rules
      rules:
        - alert: SYN_CephClusterErrorState
          annotations:
            description: Storage cluster is in error state for more than 10m.
            message: Storage cluster is in error state
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephClusterErrorState.html
            severity_level: error
            storage_type: ceph
          expr: 'ceph_health_status{job="rook-ceph-mgr"} > 1

            '
          for: 10m
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephClusterWarningState
          annotations:
            description: Storage cluster is in warning state for more than 10m.
            message: Storage cluster is in degraded state
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephClusterWarningState.html
            severity_level: warning
            storage_type: ceph
          expr: 'ceph_health_status{job="rook-ceph-mgr"} == 1

            '
          for: 15m
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephOSDVersionMismatch
          annotations:
            description: There are {{ $value }} different versions of Ceph OSD components
              running.
            message: There are multiple versions of storage services running.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephOSDVersionMismatch.html
            severity_level: warning
            storage_type: ceph
          expr: 'count(count(ceph_osd_metadata{job="rook-ceph-mgr"}) by (ceph_version,
            namespace)) by (ceph_version, namespace) > 1

            '
          for: 10m
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephMonVersionMismatch
          annotations:
            description: There are {{ $value }} different versions of Ceph Mon components
              running.
            message: There are multiple versions of storage services running.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephMonVersionMismatch.html
            severity_level: warning
            storage_type: ceph
          expr: 'count(count(ceph_mon_metadata{job="rook-ceph-mgr", ceph_version !=
            ""}) by (ceph_version)) > 1

            '
          for: 10m
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
    - name: cluster-utilization-alert.rules
      rules:
        - alert: SYN_CephClusterNearFull
          annotations:
            description: Storage cluster utilization has crossed 75% and will become
              read-only at 85%. Free up some space or expand the storage cluster.
            message: Storage cluster is nearing full. Data deletion or cluster expansion
              is required.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephClusterNearFull.html
            severity_level: warning
            storage_type: ceph
          expr: 'ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes > 0.75

            '
          for: 5s
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephClusterCriticallyFull
          annotations:
            description: Storage cluster utilization has crossed 80% and will become
              read-only at 85%. Free up some space or expand the storage cluster immediately.
            message: Storage cluster is critically full and needs immediate data deletion
              or cluster expansion.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephClusterCriticallyFull.html
            severity_level: error
            storage_type: ceph
          expr: 'ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes > 0.80

            '
          for: 5s
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
        - alert: SYN_CephClusterReadOnly
          annotations:
            description: Storage cluster utilization has crossed 85% and will become
              read-only now. Free up some space or expand the storage cluster immediately.
            message: Storage cluster is read-only now and needs immediate data deletion
              or cluster expansion.
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/CephClusterReadOnly.html
            severity_level: error
            storage_type: ceph
          expr: 'ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes >= 0.85

            '
          for: 0s
          labels:
            severity: critical
            syn: 'true'
            syn_component: rook-ceph
    - name: syn-rook-ceph-additional.alerts
      rules:
        - alert: SYN_RookCephOperatorScaledDown
          annotations:
            description: TODO
            runbook_url: https://hub.syn.tools/rook-ceph/runbooks/RookCephOperatorScaledDown.html
            summary: rook-ceph operator scaled to 0 for more than 1 hour.
          expr: kube_deployment_spec_replicas{deployment="rook-ceph-operator", namespace="syn-rook-ceph-operator"}
            == 0
          for: 1h
          labels:
            severity: warning
            syn: 'true'
            syn_component: rook-ceph
